{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9965832,"sourceType":"datasetVersion","datasetId":6130545},{"sourceId":9996806,"sourceType":"datasetVersion","datasetId":6152839}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install llama-index-vector-stores-qdrant==0.3.1 -q \n%pip install llama-index-embeddings-fastembed==0.2.0 -q  \n%pip install -qU qdrant_client==1.12.0 fastembed==0.4.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install llama-index-embeddings-huggingface -q ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**IMPORT**","metadata":{}},{"cell_type":"code","source":"import logging\nimport sys\nimport os\n\nimport qdrant_client\nfrom IPython.display import Markdown, display\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import StorageContext\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core.node_parser import HTMLNodeParser\n# bge = \"BAAI/bge-base-en-v1.5\"\nmpnet = \"sentence-transformers/all-mpnet-base-v2\"\nSettings.embed_model = HuggingFaceEmbedding(model_name=mpnet)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport pandas as pd\nimport time  \nfrom nltk.tokenize import sent_tokenize\nimport torch\nimport torch.nn.functional as F  \nimport jieba ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nest_asyncio\n\nnest_asyncio.apply()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.core.schema import Document","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"QDRANT_KEY = \"i-XYrx9YHAWP8k2xRM_R9uwZlSIZWnfz5v4SIfREBqREYb7Hc83f8Q\"\nQDRANT_URI = \"452250b2-c7ed-4c0e-bede-89cf5a18af83.us-east4-0.gcp.cloud.qdrant.io\"\nQDRANT_PORT = \"6333\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"import json\n\n# Đường dẫn tới file JSON\nfile_path = '/kaggle/input/rag-dataset/data/corpus/raw/hotpotqa.json'\n\n\nwith open(file_path, 'r', encoding='utf-8') as file:\n    data = json.load(file)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_name_or_path= 'Qwen/Qwen2.5-1.5B-Instruct'  \ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() and torch.cuda.device_count() > device_id else 'cpu')  \nsmall_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True)  \nsmall_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).to(device)   \nsmall_model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CHUNKING","metadata":{}},{"cell_type":"code","source":"def count_words(input_string):\n        words = input_string.split()\n        return round(1.2*len(words))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_text_by_punctuation(text,language): \n    if language=='zh': \n        sentences = jieba.cut(text, cut_all=False)  \n        sentences_list = list(sentences)  \n        sentences = []  \n        temp_sentence = \"\"  \n        for word in sentences_list:  \n            if word in [\"。\", \"！\", \"？\",\"；\"]:  \n                sentences.append(temp_sentence.strip()+word)  \n                temp_sentence = \"\"  \n            else:  \n                temp_sentence += word  \n        if temp_sentence:   \n            sentences.append(temp_sentence.strip())  \n        \n        return sentences\n    else:\n        full_segments = sent_tokenize(text)\n        ret = []\n        for item in full_segments:\n            item_l = item.strip().split(' ')\n            if len(item_l) > 512:\n                if len(item_l) > 1024:\n                    item = ' '.join(item_l[:256]) + \"...\"\n                else:\n                    item = ' '.join(item_l[:512]) + \"...\"\n            ret.append(item)\n        return ret\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_prob_subtract(model,tokenizer,sentence1,sentence2,language):\n    if language=='zh':\n        query='''这是一个文本分块任务.你是一位文本分析专家，请根据提供的句子的逻辑结构和语义内容，从下面两种方案中选择一种分块方式：\n        1. 将“{}”分割成“{}”与“{}”两部分；\n        2. 将“{}”不进行分割，保持原形式；\n        请回答1或2。'''.format(sentence1+sentence2,sentence1,sentence2,sentence1+sentence2)\n        prompt=\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\".format(query)\n        prompt_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n        input_ids=prompt_ids\n        output_ids = tokenizer.encode(['1','2'], return_tensors='pt').to(model.device)\n        with torch.no_grad():\n            outputs = model(input_ids)\n            next_token_logits = outputs.logits[:, -1, :]\n            token_probs = F.softmax(next_token_logits, dim=-1)\n        next_token_id_0 = output_ids[:, 0].unsqueeze(0)\n        next_token_prob_0 = token_probs[:, next_token_id_0].item()      \n        next_token_id_1 = output_ids[:, 1].unsqueeze(0)\n        next_token_prob_1 = token_probs[:, next_token_id_1].item()  \n        prob_subtract=next_token_prob_1-next_token_prob_0\n    else:\n        query='''This is a text chunking task. You are a text analysis expert. Please choose one of the following two options based on the logical structure and semantic content of the provided sentence:\n        1. Split \"{}\" into \"{}\" and \"{}\" two parts;\n        2. Keep \"{}\" unsplit in its original form;\n        Please answer 1 or 2.'''.format(sentence1+' '+sentence2,sentence1,sentence2,sentence1+' '+sentence2)\n        prompt=\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\".format(query)\n        prompt_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n        input_ids=prompt_ids\n        output_ids = tokenizer.encode(['1','2'], return_tensors='pt').to(model.device)\n        with torch.no_grad():\n            outputs = model(input_ids)\n            next_token_logits = outputs.logits[:, -1, :]\n            token_probs = F.softmax(next_token_logits, dim=-1)\n        next_token_id_0 = output_ids[:, 0].unsqueeze(0)\n        next_token_prob_0 = token_probs[:, next_token_id_0].item()      \n        next_token_id_1 = output_ids[:, 1].unsqueeze(0)\n        next_token_prob_1 = token_probs[:, next_token_id_1].item()  \n        prob_subtract=next_token_prob_1-next_token_prob_0\n    return prob_subtract","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# system_prompt = \"\"\"You will receive as input an english document with paragraphs identified by 'ID XXXX: <text>'.\n\n# Task: Find the first paragraph (not the first one) where the content clearly changes compared to the previous paragraphs.\n\n# Output: Return the ID of the paragraph with the content shift as in the exemplified format: 'Answer: ID XXXX'.\n\n# Additional Considerations: Avoid very long groups of paragraphs. Aim for a good balance between identifying content shifts and keeping groups manageable.\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def llm_chunker_ms(text,model,tokenizer,language,dynamic_merge,target_size):\n    start_time = time.time() \n    save_list=[]\n\n    threshold=0\n    threshold_list=[]\n    full_segments = split_text_by_punctuation(text,language)\n    tmp=''\n    for sentence in full_segments:\n        if tmp=='':\n            tmp+=sentence\n        else:\n            prob_subtract=get_prob_subtract(model,tokenizer,tmp,sentence,language)    \n            threshold_list.append(prob_subtract)\n            # print('222',prob_subtract)\n            if prob_subtract>threshold:\n                if language=='en':\n                    tmp+=' '+sentence\n                else:\n                    tmp+=sentence\n            else:\n                save_list.append(tmp)\n                tmp=sentence\n        if len(threshold_list)>=5:\n            last_ten = threshold_list[-5:]  \n            avg = sum(last_ten) / len(last_ten)\n            threshold=avg\n    if tmp!='':\n        save_list.append(tmp)\n    \n    new_final_chunks=save_list\n    if dynamic_merge!='no':\n        merged_paragraphs = []  \n        current_paragraph = \"\" \n        if language=='en':\n            for paragraph in new_final_chunks:  \n                # Check if adding a new paragraph to the current paragraph exceeds the target size\n                if len(current_paragraph.split()) + len(paragraph.split()) <= target_size:  \n                    current_paragraph +=' '+paragraph  \n                else:  \n                    merged_paragraphs.append(current_paragraph)  \n                    current_paragraph = paragraph  \n            if current_paragraph:  \n                merged_paragraphs.append(current_paragraph)  \n        else:\n            for paragraph in new_final_chunks:  \n                if len(current_paragraph) + len(paragraph) <= target_size:  \n                    current_paragraph +=paragraph  \n                else:  \n                    merged_paragraphs.append(current_paragraph)  \n                    current_paragraph = paragraph \n            if current_paragraph:  \n                merged_paragraphs.append(current_paragraph) \n    else:\n        merged_paragraphs = new_final_chunks\n\n        \n    end_time = time.time()   \n    execution_time = end_time - start_time  \n    print(f\"The program execution time is: {execution_time} seconds.\")\n    \n    return merged_paragraphs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.core.schema import Document","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = qdrant_client.QdrantClient(\n    host=QDRANT_URI,\n    port=QDRANT_PORT,\n    api_key=QDRANT_KEY,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aclient = qdrant_client.AsyncQdrantClient(\n    host=QDRANT_URI,\n    port=QDRANT_PORT,\n    api_key=QDRANT_KEY,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vector_store = QdrantVectorStore(\n    collection_name=\"Metachunk_hotpot_Qwen_1.5B_raw_300token\",\n    client=client,\n    aclient=aclient,\n    prefer_grpc=True,\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sub_docs(chunks):\n    \n    sub_docs = [\n        Document(\n            text=chunk[\"text\"],  # Lấy nội dung văn bản từ chunk\n            metadata={\n                # \"title\": chunk[\"title\"],\n                \"doc\": chunk['doc'],\n                \"number_chunk\":chunk['number'] \n            }  # Thêm metadata\n        )\n        for idx, chunk in enumerate(chunks)\n    ]\n    return sub_docs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language='en' # en or zh\ndynamic_merge='yes' # no or yes\ntarget_size=300 # If dynamic_merge='yes', then the chunk length value needs to be set\n# chunks=llm_chunker_ms(text,small_model,small_tokenizer,language,dynamic_merge,target_size)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_chunks =[]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count = 0\nfor ind in range(len(data)):\n    count = count+1\n    torch.cuda.empty_cache()\n    document = data[ind]['paragraph_text']\n    chunks=llm_chunker_ms(document,small_model,small_tokenizer,language,dynamic_merge,target_size) \n    chunks_after = []\n    for idx, node in enumerate(chunks):\n        chunks_after.append({\n            \"text\": chunks[idx],\n            \"doc\": ind,\n            # \"title\": data[ind]['title'], \n            \"number\": idx } # Gắn metadata vào chunk\n        )\n        all_chunks.append(chunks[idx])\n    sub_docs = get_sub_docs(chunks_after)\n    index = VectorStoreIndex.from_documents(\n    sub_docs,\n    storage_context=storage_context,\n    use_async=True,\n    )\n    print('doc', ind)\n    print(len(chunks))\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(all_chunks))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# file_path = 'chunk_raptor_part_1.txt'\n\nfile_path = 'chunks_data_meta_300token.json'\n\n# Mở file và ghi dữ liệu vào dưới dạng JSON\nwith open(file_path, 'w') as json_file:\n    json.dump(all_chunks, json_file, ensure_ascii=False, indent=4)\n\nprint(f\"Đã xuất dữ liệu vào file {file_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\nimport huggingface_hub\nhuggingface_hub.login('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')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}