{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Regex tổng hợp\n",
    "patterns = {\n",
    "    'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "    'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "    'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "    'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để trả lời chính xác liệu phương pháp trên đã \"vét đủ các cases\" chưa, cần xem xét tất cả các trường hợp tiềm năng trong đoạn văn và ngữ cảnh tương tự. Dưới đây là phân tích và kiểm tra các trường hợp có thể gặp phải trong đoạn văn mẫu của bạn:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Các dạng cụm phổ biến trong đoạn văn**\n",
    "#### a) **Tên người**\n",
    "- Dạng cơ bản: `Justice William Brennan`\n",
    "- Dạng có viết tắt: `Justice William J. Brennan`\n",
    "- Dạng có hậu tố: `Justice William J. Brennan, Jr.`\n",
    "- Các danh xưng khác: `Mr.`, `Dr.`, `Chief Justice`, `Hon.`\n",
    "- Tên riêng với nhiều từ: `Warren Earl Burger`\n",
    "\n",
    "#### b) **Tên vụ kiện**\n",
    "- Dạng chuẩn: `Miller v. California`\n",
    "- Dạng mở rộng: `Roth v. United States`\n",
    "- Dạng cổ điển: `Regina v Hicklin`\n",
    "- Dạng có chú thích trong văn bản: `Miller v. California, 413 U.S. 15 (1973)`\n",
    "\n",
    "#### c) **Cụm từ đặc biệt không phải tên**\n",
    "- Quy định pháp lý: `California Penal Code 311.2(a)`\n",
    "- Quy tắc hoặc tên thuật ngữ: `Hicklin test`, `Comstock laws`\n",
    "- Tên tác phẩm, sự kiện: `Paris Adult Theatre I v. Slaton`, `Ashcroft v. Free Speech Coalition`\n",
    "\n",
    "#### d) **Chữ viết tắt**\n",
    "- Viết tắt dạng đơn: `U.S.`, `v.`\n",
    "- Viết tắt theo kiểu học thuật hoặc pháp lý: `Roth v. United States, 354 U.S. 476 (1957)`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Những trường hợp có thể chưa xử lý đủ**\n",
    "#### **a) Tên người với nhiều phần hoặc dạng khác biệt**\n",
    "Ví dụ:\n",
    "- `Chief Justice Warren Earl Burger`\n",
    "  - Regex hiện tại có thể bỏ sót các cụm tên có nhiều từ hoặc danh xưng phức tạp.\n",
    "\n",
    "#### **b) Tên vụ kiện có số hiệu hoặc nhiều từ**\n",
    "Ví dụ:\n",
    "- `Miller v. California, 413 U.S. 15 (1973)`\n",
    "  - Regex hiện tại không nhận diện phần số hiệu (`413 U.S. 15`) đi kèm.\n",
    "\n",
    "#### **c) Các danh từ viết hoa không thuộc nhóm tên người hoặc vụ kiện**\n",
    "Ví dụ:\n",
    "- `Hicklin test`, `Roth test`\n",
    "  - Cần bổ sung nếu muốn bảo toàn các thuật ngữ pháp lý.\n",
    "\n",
    "#### **d) Các cụm viết tắt đặc biệt**\n",
    "Ví dụ:\n",
    "- `v.` (trong \"Miller v. California\") hoặc `U.S.`\n",
    "  - Regex hiện tại dễ nhầm lẫn `v.` hoặc `U.S.` với dấu chấm kết thúc câu.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Điều chỉnh để vét đủ các cases**\n",
    "#### **Regex cập nhật cho từng nhóm:**\n",
    "1. **Tên người (bao gồm danh xưng, tên dài):**\n",
    "   ```regex\n",
    "   \\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?\n",
    "   ```\n",
    "   - Xử lý được cả danh xưng phức tạp như `Chief Justice Warren Earl Burger`.\n",
    "\n",
    "2. **Tên vụ kiện (có số hiệu hoặc mô tả):**\n",
    "   ```regex\n",
    "   \\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?\n",
    "   ```\n",
    "   - Nhận diện tên vụ kiện kèm số hiệu như `Roth v. United States, 354 U.S. 476 (1957)`.\n",
    "\n",
    "3. **Các cụm thuật ngữ đặc biệt:**\n",
    "   ```regex\n",
    "   \\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b\n",
    "   ```\n",
    "   - Ví dụ: `Hicklin test`, `Comstock laws`.\n",
    "\n",
    "4. **Viết tắt pháp lý hoặc học thuật:**\n",
    "   ```regex\n",
    "   \\b(?:[A-Z]\\.)(?:[A-Z]\\.)?\n",
    "   ```\n",
    "   - Nhận diện viết tắt như `U.S.` hoặc `v.` trong ngữ cảnh cụ thể.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Phương pháp tổng hợp**\n",
    "1. Nhận diện tất cả các cụm theo các regex trên.\n",
    "2. Thay thế tạm thời bằng placeholder để bảo toàn.\n",
    "3. Thực hiện tách câu dựa trên dấu chấm (`.`).\n",
    "4. Khôi phục các cụm gốc từ placeholder.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Mã Python hoàn chỉnh**\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Văn bản mẫu\n",
    "text = \"\"\"Justice William J. Brennan, Jr., led the charge for protecting \"obscenity\". \n",
    "Miller v. California, 413 U.S. 15 (1973), was a landmark case. Chief Justice Warren Earl Burger pushed for looser definitions.\"\"\"\n",
    "\n",
    "# Regex tổng hợp\n",
    "patterns = {\n",
    "    'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "    'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "    'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "    'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?'\n",
    "}\n",
    "\n",
    "# Placeholder dictionaries\n",
    "placeholders = {}\n",
    "\n",
    "# Thay thế các cụm nhận diện bằng placeholder\n",
    "for label, pattern in patterns.items():\n",
    "    matches = re.findall(pattern, text)\n",
    "    for i, match in enumerate(matches):\n",
    "        placeholder = f\"PLACEHOLDER_{label.upper()}_{i}\"\n",
    "        placeholders[placeholder] = match\n",
    "        text = text.replace(match, placeholder)\n",
    "\n",
    "# Tách câu\n",
    "sentences = re.split(r'\\.(?=\\s+[A-Z])', text)\n",
    "\n",
    "# Khôi phục các cụm gốc\n",
    "final_sentences = [sentence.strip() for sentence in sentences]\n",
    "for placeholder, original in placeholders.items():\n",
    "    final_sentences = [sentence.replace(placeholder, original) for sentence in final_sentences]\n",
    "\n",
    "# Kết quả\n",
    "for sentence in final_sentences:\n",
    "    print(sentence)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Đảm bảo \"vét đủ cases\"**\n",
    "Với các regex và phương pháp trên:\n",
    "- **Tên người:** Bao gồm tất cả dạng phức tạp như danh xưng dài, viết tắt, hậu tố.\n",
    "- **Tên vụ kiện:** Được nhận diện cả số hiệu, năm.\n",
    "- **Thuật ngữ đặc biệt:** Đảm bảo giữ nguyên các cụm quan trọng như \"Hicklin test\".\n",
    "- **Viết tắt:** Không nhầm lẫn với dấu câu.\n",
    "\n",
    "Nếu bạn phát hiện thêm trường hợp chưa được xử lý, hãy chia sẻ để mình bổ sung! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# import tiktoken\n",
    "# import re\n",
    "# def split_text(\n",
    "#     text: str, tokenizer: tiktoken.get_encoding(\"cl100k_base\"), max_tokens: int, overlap: int = 0\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Splits the input text into smaller chunks based on the tokenizer and maximum allowed tokens.\n",
    "    \n",
    "#     Args:\n",
    "#         text (str): The text to be split.\n",
    "#         tokenizer (CustomTokenizer): The tokenizer to be used for splitting the text.\n",
    "#         max_tokens (int): The maximum allowed tokens.\n",
    "#         overlap (int, optional): The number of overlapping tokens between chunks. Defaults to 0.\n",
    "    \n",
    "#     Returns:\n",
    "#         List[str]: A list of text chunks.\n",
    "#     \"\"\"\n",
    "#     # Split the text into sentences using multiple delimiters\n",
    "#     delimiters = [\".\", \"!\", \"?\", \"\\n\"]\n",
    "#     regex_pattern = \"|\".join(map(re.escape, delimiters))\n",
    "#     sentences = re.split(regex_pattern, text)\n",
    "#     # print(sentences)\n",
    "#     # Calculate the number of tokens for each sentence\n",
    "#     n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "#     chunks = []\n",
    "#     current_chunk = []\n",
    "#     current_length = 0\n",
    "    \n",
    "#     for sentence, token_count in zip(sentences, n_tokens):\n",
    "#         # If the sentence is empty or consists only of whitespace, skip it\n",
    "#         if not sentence.strip():\n",
    "#             continue\n",
    "        \n",
    "#         # If the sentence is too long, split it into smaller parts\n",
    "#         if token_count > max_tokens:\n",
    "#             sub_sentences = re.split(r\"[,;:]\", sentence)\n",
    "            \n",
    "#             # there is no need to keep empty os only-spaced strings\n",
    "#             # since spaces will be inserted in the beginning of the full string\n",
    "#             # and in between the string in the sub_chuk list\n",
    "#             filtered_sub_sentences = [sub.strip() for sub in sub_sentences if sub.strip() != \"\"]\n",
    "#             sub_token_counts = [len(tokenizer.encode(\" \" + sub_sentence)) for sub_sentence in filtered_sub_sentences]\n",
    "            \n",
    "#             sub_chunk = []\n",
    "#             sub_length = 0\n",
    "            \n",
    "#             for sub_sentence, sub_token_count in zip(filtered_sub_sentences, sub_token_counts):\n",
    "#                 if sub_length + sub_token_count > max_tokens:\n",
    "                    \n",
    "#                     # if the phrase does not have sub_sentences, it would create an empty chunk\n",
    "#                     # this big phrase would be added anyways in the next chunk append\n",
    "#                     if sub_chunk:\n",
    "#                         chunks.append(\" \".join(sub_chunk))\n",
    "#                         sub_chunk = sub_chunk[-overlap:] if overlap > 0 else []\n",
    "#                         sub_length = sum(sub_token_counts[max(0, len(sub_chunk) - overlap):len(sub_chunk)])\n",
    "                \n",
    "#                 sub_chunk.append(sub_sentence)\n",
    "#                 sub_length += sub_token_count\n",
    "            \n",
    "#             if sub_chunk:\n",
    "#                 chunks.append(\" \".join(sub_chunk))\n",
    "        \n",
    "#         # If adding the sentence to the current chunk exceeds the max tokens, start a new chunk\n",
    "#         elif current_length + token_count > max_tokens:\n",
    "#             chunks.append(\" \".join(current_chunk))\n",
    "#             current_chunk = current_chunk[-overlap:] if overlap > 0 else []\n",
    "#             current_length = sum(n_tokens[max(0, len(current_chunk) - overlap):len(current_chunk)])\n",
    "#             current_chunk.append(sentence)\n",
    "#             current_length += token_count\n",
    "        \n",
    "#         # Otherwise, add the sentence to the current chunk\n",
    "#         else:\n",
    "#             current_chunk.append(sentence)\n",
    "#             current_length += token_count\n",
    "    \n",
    "#     # Add the last chunk if it's not empty\n",
    "#     if current_chunk:\n",
    "#         chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tiktoken\n",
    "import re\n",
    "\n",
    "def process_patterns(text, patterns):\n",
    "    \"\"\"\n",
    "    Xử lý các cụm đặc biệt trong văn bản dựa trên các regex patterns, thay thế chúng bằng placeholders.\n",
    "\n",
    "    Args:\n",
    "        text (str): Văn bản đầu vào.\n",
    "        patterns (dict): Từ điển chứa các regex patterns.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, dict]: Văn bản đã thay thế placeholders và từ điển chứa mapping từ placeholder đến cụm gốc.\n",
    "    \"\"\"\n",
    "    placeholders = {}\n",
    "    for label, pattern in patterns.items():\n",
    "        # Tìm tất cả các cụm khớp với pattern\n",
    "        matches = re.findall(pattern, text)\n",
    "        for i, match in enumerate(matches):\n",
    "            # Tạo placeholder để thay thế cụm khớp\n",
    "            placeholder = f\"PLACEHOLDER_{label.upper()}_{i}\"\n",
    "            # Lấy giá trị khớp đầy đủ\n",
    "            full_match = match if isinstance(match, str) else match[0]\n",
    "            # Lưu giá trị khớp vào dictionary để khôi phục sau\n",
    "            placeholders[placeholder] = full_match\n",
    "            # Thay thế cụm khớp trong văn bản bằng placeholder\n",
    "            text = text.replace(full_match, placeholder, 1)\n",
    "    return text, placeholders\n",
    "\n",
    "    # Định nghĩa các mẫu regex để nhận diện các cụm đặc biệt\n",
    "    patterns = {\n",
    "        'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "        'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "        'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "        'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?',\n",
    "        'legal_code': r'\\b(?:[A-Z][a-z]+)\\s+\\d+(\\.\\d+)?(?:\\([a-zA-Z0-9]+\\))?'\n",
    "    }\n",
    "\n",
    "\n",
    "def split_text(\n",
    "    text: str, tokenizer: tiktoken.get_encoding(\"cl100k_base\"), max_tokens: int, overlap: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Chia nhỏ văn bản đầu vào thành các đoạn (chunks) dựa trên số lượng tokens tối đa được phép.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Văn bản cần được chia nhỏ.\n",
    "        tokenizer (CustomTokenizer): Tokenizer dùng để mã hóa văn bản.\n",
    "        max_tokens (int): Số lượng tokens tối đa trong mỗi đoạn.\n",
    "        overlap (int, optional): Số câu trùng lặp giữa các đoạn. Mặc định là 0.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Danh sách các đoạn văn bản đã được chia nhỏ.\n",
    "    \"\"\"\n",
    "    # Định nghĩa các mẫu regex để nhận diện các cụm đặc biệt\n",
    "    patterns = {\n",
    "        'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "        'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "        'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "        'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?',\n",
    "        'legal_code': r'\\b(?:[A-Z][a-z]+)\\s+\\d+(\\.\\d+)?(?:\\([a-zA-Z0-9]+\\))?'\n",
    "    }\n",
    "\n",
    "    # Gọi hàm xử lý patterns để thay thế các cụm đặc biệt bằng placeholders\n",
    "    text, placeholders = process_patterns(text, patterns)\n",
    "    \n",
    "    # Tách văn bản thành các câu dựa trên dấu câu\n",
    "    sentence_pattern = r'(?<=[.!?])\\s+'\n",
    "    sentences = re.split(sentence_pattern, text)\n",
    "    \n",
    "    # Khôi phục lại các placeholders về nội dung gốc trong từng câu\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for placeholder, original in placeholders.items():\n",
    "            # Thay thế placeholders bằng nội dung gốc\n",
    "            sentences[i] = sentences[i].replace(placeholder, original)\n",
    "    \n",
    "    # Tính toán số lượng tokens trong từng câu\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    # Chia văn bản thành các đoạn (chunks) dựa trên số tokens tối đa\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i, (sentence, token_count) in enumerate(zip(sentences, n_tokens)):\n",
    "        # Nếu thêm câu này vào sẽ vượt quá số tokens tối đa, tạo chunk mới\n",
    "        if current_length + token_count > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))  # Thêm chunk hiện tại vào danh sách\n",
    "            # Chỉ giữ lại số câu overlap ở cuối chunk hiện tại cho chunk tiếp theo\n",
    "            current_chunk = current_chunk[-overlap:] if overlap > 0 else []\n",
    "            # Tính lại số tokens của phần overlap\n",
    "            current_length = sum(\n",
    "                [len(tokenizer.encode(\" \" + chunk)) for chunk in current_chunk]\n",
    "            )\n",
    "        \n",
    "        # Thêm câu hiện tại vào chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += token_count\n",
    "    \n",
    "    # Thêm chunk cuối cùng nếu không rỗng\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "text = \"\"\"\n",
    "Justice William J. Brennan, Jr., led the charge for protecting \"obscenity\". \n",
    "Miller v. California, 413 U.S. 15 (1973), was a landmark case. Chief Justice Warren Earl Burger pushed for looser definitions.\n",
    "California Penal Code 311.2(a) specifies certain legal restrictions.\n",
    "\"\"\"\n",
    "# Tạo tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# Gọi hàm để chia văn bản\n",
    "chunks = split_text(text, tokenizer, max_tokens=50, overlap=1)\n",
    "\n",
    "# In kết quả\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "chunk trước rồi mới KHÔI PHỤC \n",
    "\n",
    "    # Khôi phục lại các placeholders về nội dung gốc trong từng câu\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for placeholder, original in placeholders.items():\n",
    "            # Thay thế placeholders bằng nội dung gốc\n",
    "            sentences[i] = sentences[i].replace(placeholder, original)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tư duy xài thư viện NLP đã có "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
