{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Regex tá»•ng há»£p\n",
    "patterns = {\n",
    "    'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "    'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "    'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "    'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Äá»ƒ tráº£ lá»i chÃ­nh xÃ¡c liá»‡u phÆ°Æ¡ng phÃ¡p trÃªn Ä‘Ã£ \"vÃ©t Ä‘á»§ cÃ¡c cases\" chÆ°a, cáº§n xem xÃ©t táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p tiá»m nÄƒng trong Ä‘oáº¡n vÄƒn vÃ  ngá»¯ cáº£nh tÆ°Æ¡ng tá»±. DÆ°á»›i Ä‘Ã¢y lÃ  phÃ¢n tÃ­ch vÃ  kiá»ƒm tra cÃ¡c trÆ°á»ng há»£p cÃ³ thá»ƒ gáº·p pháº£i trong Ä‘oáº¡n vÄƒn máº«u cá»§a báº¡n:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. CÃ¡c dáº¡ng cá»¥m phá»• biáº¿n trong Ä‘oáº¡n vÄƒn**\n",
    "#### a) **TÃªn ngÆ°á»i**\n",
    "- Dáº¡ng cÆ¡ báº£n: `Justice William Brennan`\n",
    "- Dáº¡ng cÃ³ viáº¿t táº¯t: `Justice William J. Brennan`\n",
    "- Dáº¡ng cÃ³ háº­u tá»‘: `Justice William J. Brennan, Jr.`\n",
    "- CÃ¡c danh xÆ°ng khÃ¡c: `Mr.`, `Dr.`, `Chief Justice`, `Hon.`\n",
    "- TÃªn riÃªng vá»›i nhiá»u tá»«: `Warren Earl Burger`\n",
    "\n",
    "#### b) **TÃªn vá»¥ kiá»‡n**\n",
    "- Dáº¡ng chuáº©n: `Miller v. California`\n",
    "- Dáº¡ng má»Ÿ rá»™ng: `Roth v. United States`\n",
    "- Dáº¡ng cá»• Ä‘iá»ƒn: `Regina v Hicklin`\n",
    "- Dáº¡ng cÃ³ chÃº thÃ­ch trong vÄƒn báº£n: `Miller v. California, 413 U.S. 15 (1973)`\n",
    "\n",
    "#### c) **Cá»¥m tá»« Ä‘áº·c biá»‡t khÃ´ng pháº£i tÃªn**\n",
    "- Quy Ä‘á»‹nh phÃ¡p lÃ½: `California Penal Code 311.2(a)`\n",
    "- Quy táº¯c hoáº·c tÃªn thuáº­t ngá»¯: `Hicklin test`, `Comstock laws`\n",
    "- TÃªn tÃ¡c pháº©m, sá»± kiá»‡n: `Paris Adult Theatre I v. Slaton`, `Ashcroft v. Free Speech Coalition`\n",
    "\n",
    "#### d) **Chá»¯ viáº¿t táº¯t**\n",
    "- Viáº¿t táº¯t dáº¡ng Ä‘Æ¡n: `U.S.`, `v.`\n",
    "- Viáº¿t táº¯t theo kiá»ƒu há»c thuáº­t hoáº·c phÃ¡p lÃ½: `Roth v. United States, 354 U.S. 476 (1957)`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Nhá»¯ng trÆ°á»ng há»£p cÃ³ thá»ƒ chÆ°a xá»­ lÃ½ Ä‘á»§**\n",
    "#### **a) TÃªn ngÆ°á»i vá»›i nhiá»u pháº§n hoáº·c dáº¡ng khÃ¡c biá»‡t**\n",
    "VÃ­ dá»¥:\n",
    "- `Chief Justice Warren Earl Burger`\n",
    "  - Regex hiá»‡n táº¡i cÃ³ thá»ƒ bá» sÃ³t cÃ¡c cá»¥m tÃªn cÃ³ nhiá»u tá»« hoáº·c danh xÆ°ng phá»©c táº¡p.\n",
    "\n",
    "#### **b) TÃªn vá»¥ kiá»‡n cÃ³ sá»‘ hiá»‡u hoáº·c nhiá»u tá»«**\n",
    "VÃ­ dá»¥:\n",
    "- `Miller v. California, 413 U.S. 15 (1973)`\n",
    "  - Regex hiá»‡n táº¡i khÃ´ng nháº­n diá»‡n pháº§n sá»‘ hiá»‡u (`413 U.S. 15`) Ä‘i kÃ¨m.\n",
    "\n",
    "#### **c) CÃ¡c danh tá»« viáº¿t hoa khÃ´ng thuá»™c nhÃ³m tÃªn ngÆ°á»i hoáº·c vá»¥ kiá»‡n**\n",
    "VÃ­ dá»¥:\n",
    "- `Hicklin test`, `Roth test`\n",
    "  - Cáº§n bá»• sung náº¿u muá»‘n báº£o toÃ n cÃ¡c thuáº­t ngá»¯ phÃ¡p lÃ½.\n",
    "\n",
    "#### **d) CÃ¡c cá»¥m viáº¿t táº¯t Ä‘áº·c biá»‡t**\n",
    "VÃ­ dá»¥:\n",
    "- `v.` (trong \"Miller v. California\") hoáº·c `U.S.`\n",
    "  - Regex hiá»‡n táº¡i dá»… nháº§m láº«n `v.` hoáº·c `U.S.` vá»›i dáº¥u cháº¥m káº¿t thÃºc cÃ¢u.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Äiá»u chá»‰nh Ä‘á»ƒ vÃ©t Ä‘á»§ cÃ¡c cases**\n",
    "#### **Regex cáº­p nháº­t cho tá»«ng nhÃ³m:**\n",
    "1. **TÃªn ngÆ°á»i (bao gá»“m danh xÆ°ng, tÃªn dÃ i):**\n",
    "   ```regex\n",
    "   \\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?\n",
    "   ```\n",
    "   - Xá»­ lÃ½ Ä‘Æ°á»£c cáº£ danh xÆ°ng phá»©c táº¡p nhÆ° `Chief Justice Warren Earl Burger`.\n",
    "\n",
    "2. **TÃªn vá»¥ kiá»‡n (cÃ³ sá»‘ hiá»‡u hoáº·c mÃ´ táº£):**\n",
    "   ```regex\n",
    "   \\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?\n",
    "   ```\n",
    "   - Nháº­n diá»‡n tÃªn vá»¥ kiá»‡n kÃ¨m sá»‘ hiá»‡u nhÆ° `Roth v. United States, 354 U.S. 476 (1957)`.\n",
    "\n",
    "3. **CÃ¡c cá»¥m thuáº­t ngá»¯ Ä‘áº·c biá»‡t:**\n",
    "   ```regex\n",
    "   \\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b\n",
    "   ```\n",
    "   - VÃ­ dá»¥: `Hicklin test`, `Comstock laws`.\n",
    "\n",
    "4. **Viáº¿t táº¯t phÃ¡p lÃ½ hoáº·c há»c thuáº­t:**\n",
    "   ```regex\n",
    "   \\b(?:[A-Z]\\.)(?:[A-Z]\\.)?\n",
    "   ```\n",
    "   - Nháº­n diá»‡n viáº¿t táº¯t nhÆ° `U.S.` hoáº·c `v.` trong ngá»¯ cáº£nh cá»¥ thá»ƒ.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. PhÆ°Æ¡ng phÃ¡p tá»•ng há»£p**\n",
    "1. Nháº­n diá»‡n táº¥t cáº£ cÃ¡c cá»¥m theo cÃ¡c regex trÃªn.\n",
    "2. Thay tháº¿ táº¡m thá»i báº±ng placeholder Ä‘á»ƒ báº£o toÃ n.\n",
    "3. Thá»±c hiá»‡n tÃ¡ch cÃ¢u dá»±a trÃªn dáº¥u cháº¥m (`.`).\n",
    "4. KhÃ´i phá»¥c cÃ¡c cá»¥m gá»‘c tá»« placeholder.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. MÃ£ Python hoÃ n chá»‰nh**\n",
    "```python\n",
    "import re\n",
    "\n",
    "# VÄƒn báº£n máº«u\n",
    "text = \"\"\"Justice William J. Brennan, Jr., led the charge for protecting \"obscenity\". \n",
    "Miller v. California, 413 U.S. 15 (1973), was a landmark case. Chief Justice Warren Earl Burger pushed for looser definitions.\"\"\"\n",
    "\n",
    "# Regex tá»•ng há»£p\n",
    "patterns = {\n",
    "    'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "    'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "    'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "    'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?'\n",
    "}\n",
    "\n",
    "# Placeholder dictionaries\n",
    "placeholders = {}\n",
    "\n",
    "# Thay tháº¿ cÃ¡c cá»¥m nháº­n diá»‡n báº±ng placeholder\n",
    "for label, pattern in patterns.items():\n",
    "    matches = re.findall(pattern, text)\n",
    "    for i, match in enumerate(matches):\n",
    "        placeholder = f\"PLACEHOLDER_{label.upper()}_{i}\"\n",
    "        placeholders[placeholder] = match\n",
    "        text = text.replace(match, placeholder)\n",
    "\n",
    "# TÃ¡ch cÃ¢u\n",
    "sentences = re.split(r'\\.(?=\\s+[A-Z])', text)\n",
    "\n",
    "# KhÃ´i phá»¥c cÃ¡c cá»¥m gá»‘c\n",
    "final_sentences = [sentence.strip() for sentence in sentences]\n",
    "for placeholder, original in placeholders.items():\n",
    "    final_sentences = [sentence.replace(placeholder, original) for sentence in final_sentences]\n",
    "\n",
    "# Káº¿t quáº£\n",
    "for sentence in final_sentences:\n",
    "    print(sentence)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Äáº£m báº£o \"vÃ©t Ä‘á»§ cases\"**\n",
    "Vá»›i cÃ¡c regex vÃ  phÆ°Æ¡ng phÃ¡p trÃªn:\n",
    "- **TÃªn ngÆ°á»i:** Bao gá»“m táº¥t cáº£ dáº¡ng phá»©c táº¡p nhÆ° danh xÆ°ng dÃ i, viáº¿t táº¯t, háº­u tá»‘.\n",
    "- **TÃªn vá»¥ kiá»‡n:** ÄÆ°á»£c nháº­n diá»‡n cáº£ sá»‘ hiá»‡u, nÄƒm.\n",
    "- **Thuáº­t ngá»¯ Ä‘áº·c biá»‡t:** Äáº£m báº£o giá»¯ nguyÃªn cÃ¡c cá»¥m quan trá»ng nhÆ° \"Hicklin test\".\n",
    "- **Viáº¿t táº¯t:** KhÃ´ng nháº§m láº«n vá»›i dáº¥u cÃ¢u.\n",
    "\n",
    "Náº¿u báº¡n phÃ¡t hiá»‡n thÃªm trÆ°á»ng há»£p chÆ°a Ä‘Æ°á»£c xá»­ lÃ½, hÃ£y chia sáº» Ä‘á»ƒ mÃ¬nh bá»• sung! ğŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# import tiktoken\n",
    "# import re\n",
    "# def split_text(\n",
    "#     text: str, tokenizer: tiktoken.get_encoding(\"cl100k_base\"), max_tokens: int, overlap: int = 0\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Splits the input text into smaller chunks based on the tokenizer and maximum allowed tokens.\n",
    "    \n",
    "#     Args:\n",
    "#         text (str): The text to be split.\n",
    "#         tokenizer (CustomTokenizer): The tokenizer to be used for splitting the text.\n",
    "#         max_tokens (int): The maximum allowed tokens.\n",
    "#         overlap (int, optional): The number of overlapping tokens between chunks. Defaults to 0.\n",
    "    \n",
    "#     Returns:\n",
    "#         List[str]: A list of text chunks.\n",
    "#     \"\"\"\n",
    "#     # Split the text into sentences using multiple delimiters\n",
    "#     delimiters = [\".\", \"!\", \"?\", \"\\n\"]\n",
    "#     regex_pattern = \"|\".join(map(re.escape, delimiters))\n",
    "#     sentences = re.split(regex_pattern, text)\n",
    "#     # print(sentences)\n",
    "#     # Calculate the number of tokens for each sentence\n",
    "#     n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "#     chunks = []\n",
    "#     current_chunk = []\n",
    "#     current_length = 0\n",
    "    \n",
    "#     for sentence, token_count in zip(sentences, n_tokens):\n",
    "#         # If the sentence is empty or consists only of whitespace, skip it\n",
    "#         if not sentence.strip():\n",
    "#             continue\n",
    "        \n",
    "#         # If the sentence is too long, split it into smaller parts\n",
    "#         if token_count > max_tokens:\n",
    "#             sub_sentences = re.split(r\"[,;:]\", sentence)\n",
    "            \n",
    "#             # there is no need to keep empty os only-spaced strings\n",
    "#             # since spaces will be inserted in the beginning of the full string\n",
    "#             # and in between the string in the sub_chuk list\n",
    "#             filtered_sub_sentences = [sub.strip() for sub in sub_sentences if sub.strip() != \"\"]\n",
    "#             sub_token_counts = [len(tokenizer.encode(\" \" + sub_sentence)) for sub_sentence in filtered_sub_sentences]\n",
    "            \n",
    "#             sub_chunk = []\n",
    "#             sub_length = 0\n",
    "            \n",
    "#             for sub_sentence, sub_token_count in zip(filtered_sub_sentences, sub_token_counts):\n",
    "#                 if sub_length + sub_token_count > max_tokens:\n",
    "                    \n",
    "#                     # if the phrase does not have sub_sentences, it would create an empty chunk\n",
    "#                     # this big phrase would be added anyways in the next chunk append\n",
    "#                     if sub_chunk:\n",
    "#                         chunks.append(\" \".join(sub_chunk))\n",
    "#                         sub_chunk = sub_chunk[-overlap:] if overlap > 0 else []\n",
    "#                         sub_length = sum(sub_token_counts[max(0, len(sub_chunk) - overlap):len(sub_chunk)])\n",
    "                \n",
    "#                 sub_chunk.append(sub_sentence)\n",
    "#                 sub_length += sub_token_count\n",
    "            \n",
    "#             if sub_chunk:\n",
    "#                 chunks.append(\" \".join(sub_chunk))\n",
    "        \n",
    "#         # If adding the sentence to the current chunk exceeds the max tokens, start a new chunk\n",
    "#         elif current_length + token_count > max_tokens:\n",
    "#             chunks.append(\" \".join(current_chunk))\n",
    "#             current_chunk = current_chunk[-overlap:] if overlap > 0 else []\n",
    "#             current_length = sum(n_tokens[max(0, len(current_chunk) - overlap):len(current_chunk)])\n",
    "#             current_chunk.append(sentence)\n",
    "#             current_length += token_count\n",
    "        \n",
    "#         # Otherwise, add the sentence to the current chunk\n",
    "#         else:\n",
    "#             current_chunk.append(sentence)\n",
    "#             current_length += token_count\n",
    "    \n",
    "#     # Add the last chunk if it's not empty\n",
    "#     if current_chunk:\n",
    "#         chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tiktoken\n",
    "import re\n",
    "\n",
    "def process_patterns(text, patterns):\n",
    "    \"\"\"\n",
    "    Xá»­ lÃ½ cÃ¡c cá»¥m Ä‘áº·c biá»‡t trong vÄƒn báº£n dá»±a trÃªn cÃ¡c regex patterns, thay tháº¿ chÃºng báº±ng placeholders.\n",
    "\n",
    "    Args:\n",
    "        text (str): VÄƒn báº£n Ä‘áº§u vÃ o.\n",
    "        patterns (dict): Tá»« Ä‘iá»ƒn chá»©a cÃ¡c regex patterns.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, dict]: VÄƒn báº£n Ä‘Ã£ thay tháº¿ placeholders vÃ  tá»« Ä‘iá»ƒn chá»©a mapping tá»« placeholder Ä‘áº¿n cá»¥m gá»‘c.\n",
    "    \"\"\"\n",
    "    placeholders = {}\n",
    "    for label, pattern in patterns.items():\n",
    "        # TÃ¬m táº¥t cáº£ cÃ¡c cá»¥m khá»›p vá»›i pattern\n",
    "        matches = re.findall(pattern, text)\n",
    "        for i, match in enumerate(matches):\n",
    "            # Táº¡o placeholder Ä‘á»ƒ thay tháº¿ cá»¥m khá»›p\n",
    "            placeholder = f\"PLACEHOLDER_{label.upper()}_{i}\"\n",
    "            # Láº¥y giÃ¡ trá»‹ khá»›p Ä‘áº§y Ä‘á»§\n",
    "            full_match = match if isinstance(match, str) else match[0]\n",
    "            # LÆ°u giÃ¡ trá»‹ khá»›p vÃ o dictionary Ä‘á»ƒ khÃ´i phá»¥c sau\n",
    "            placeholders[placeholder] = full_match\n",
    "            # Thay tháº¿ cá»¥m khá»›p trong vÄƒn báº£n báº±ng placeholder\n",
    "            text = text.replace(full_match, placeholder, 1)\n",
    "    return text, placeholders\n",
    "\n",
    "    # Äá»‹nh nghÄ©a cÃ¡c máº«u regex Ä‘á»ƒ nháº­n diá»‡n cÃ¡c cá»¥m Ä‘áº·c biá»‡t\n",
    "    patterns = {\n",
    "        'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "        'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "        'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "        'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?',\n",
    "        'legal_code': r'\\b(?:[A-Z][a-z]+)\\s+\\d+(\\.\\d+)?(?:\\([a-zA-Z0-9]+\\))?'\n",
    "    }\n",
    "\n",
    "\n",
    "def split_text(\n",
    "    text: str, tokenizer: tiktoken.get_encoding(\"cl100k_base\"), max_tokens: int, overlap: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Chia nhá» vÄƒn báº£n Ä‘áº§u vÃ o thÃ nh cÃ¡c Ä‘oáº¡n (chunks) dá»±a trÃªn sá»‘ lÆ°á»£ng tokens tá»‘i Ä‘a Ä‘Æ°á»£c phÃ©p.\n",
    "    \n",
    "    Args:\n",
    "        text (str): VÄƒn báº£n cáº§n Ä‘Æ°á»£c chia nhá».\n",
    "        tokenizer (CustomTokenizer): Tokenizer dÃ¹ng Ä‘á»ƒ mÃ£ hÃ³a vÄƒn báº£n.\n",
    "        max_tokens (int): Sá»‘ lÆ°á»£ng tokens tá»‘i Ä‘a trong má»—i Ä‘oáº¡n.\n",
    "        overlap (int, optional): Sá»‘ cÃ¢u trÃ¹ng láº·p giá»¯a cÃ¡c Ä‘oáº¡n. Máº·c Ä‘á»‹nh lÃ  0.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c chia nhá».\n",
    "    \"\"\"\n",
    "    # Äá»‹nh nghÄ©a cÃ¡c máº«u regex Ä‘á»ƒ nháº­n diá»‡n cÃ¡c cá»¥m Ä‘áº·c biá»‡t\n",
    "    patterns = {\n",
    "        'person': r'\\b(?:Justice|Mr\\.|Dr\\.|Chief Justice|Hon\\.)\\s+(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)(?:\\s+[A-Z]\\.)?(?:,\\s*(Jr\\.|Sr\\.|III|IV))?',\n",
    "        'case': r'\\b[A-Z][a-zA-Z]*\\s+v\\.\\s+[A-Z][a-zA-Z]*(?:,\\s*\\d+\\s+[A-Z]{1,3}\\.\\s*\\d+\\s*\\(\\d{4}\\))?',\n",
    "        'term': r'\\b(?:[A-Z][a-z]+(?:\\s+[a-z]+)*\\s+[Tt]est|[A-Z][a-z]+\\s+[Ll]aws)\\b',\n",
    "        'abbreviation': r'\\b(?:[A-Z]\\.)(?:[A-Z]\\.)?',\n",
    "        'legal_code': r'\\b(?:[A-Z][a-z]+)\\s+\\d+(\\.\\d+)?(?:\\([a-zA-Z0-9]+\\))?'\n",
    "    }\n",
    "\n",
    "    # Gá»i hÃ m xá»­ lÃ½ patterns Ä‘á»ƒ thay tháº¿ cÃ¡c cá»¥m Ä‘áº·c biá»‡t báº±ng placeholders\n",
    "    text, placeholders = process_patterns(text, patterns)\n",
    "    \n",
    "    # TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u dá»±a trÃªn dáº¥u cÃ¢u\n",
    "    sentence_pattern = r'(?<=[.!?])\\s+'\n",
    "    sentences = re.split(sentence_pattern, text)\n",
    "    \n",
    "    # KhÃ´i phá»¥c láº¡i cÃ¡c placeholders vá» ná»™i dung gá»‘c trong tá»«ng cÃ¢u\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for placeholder, original in placeholders.items():\n",
    "            # Thay tháº¿ placeholders báº±ng ná»™i dung gá»‘c\n",
    "            sentences[i] = sentences[i].replace(placeholder, original)\n",
    "    \n",
    "    # TÃ­nh toÃ¡n sá»‘ lÆ°á»£ng tokens trong tá»«ng cÃ¢u\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    # Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n (chunks) dá»±a trÃªn sá»‘ tokens tá»‘i Ä‘a\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i, (sentence, token_count) in enumerate(zip(sentences, n_tokens)):\n",
    "        # Náº¿u thÃªm cÃ¢u nÃ y vÃ o sáº½ vÆ°á»£t quÃ¡ sá»‘ tokens tá»‘i Ä‘a, táº¡o chunk má»›i\n",
    "        if current_length + token_count > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))  # ThÃªm chunk hiá»‡n táº¡i vÃ o danh sÃ¡ch\n",
    "            # Chá»‰ giá»¯ láº¡i sá»‘ cÃ¢u overlap á»Ÿ cuá»‘i chunk hiá»‡n táº¡i cho chunk tiáº¿p theo\n",
    "            current_chunk = current_chunk[-overlap:] if overlap > 0 else []\n",
    "            # TÃ­nh láº¡i sá»‘ tokens cá»§a pháº§n overlap\n",
    "            current_length = sum(\n",
    "                [len(tokenizer.encode(\" \" + chunk)) for chunk in current_chunk]\n",
    "            )\n",
    "        \n",
    "        # ThÃªm cÃ¢u hiá»‡n táº¡i vÃ o chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += token_count\n",
    "    \n",
    "    # ThÃªm chunk cuá»‘i cÃ¹ng náº¿u khÃ´ng rá»—ng\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng\n",
    "text = \"\"\"\n",
    "Justice William J. Brennan, Jr., led the charge for protecting \"obscenity\". \n",
    "Miller v. California, 413 U.S. 15 (1973), was a landmark case. Chief Justice Warren Earl Burger pushed for looser definitions.\n",
    "California Penal Code 311.2(a) specifies certain legal restrictions.\n",
    "\"\"\"\n",
    "# Táº¡o tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# Gá»i hÃ m Ä‘á»ƒ chia vÄƒn báº£n\n",
    "chunks = split_text(text, tokenizer, max_tokens=50, overlap=1)\n",
    "\n",
    "# In káº¿t quáº£\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "chunk trÆ°á»›c rá»“i má»›i KHÃ”I PHá»¤C \n",
    "\n",
    "    # KhÃ´i phá»¥c láº¡i cÃ¡c placeholders vá» ná»™i dung gá»‘c trong tá»«ng cÃ¢u\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for placeholder, original in placeholders.items():\n",
    "            # Thay tháº¿ placeholders báº±ng ná»™i dung gá»‘c\n",
    "            sentences[i] = sentences[i].replace(placeholder, original)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TÆ° duy xÃ i thÆ° viá»‡n NLP Ä‘Ã£ cÃ³ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
